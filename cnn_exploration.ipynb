{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Convolutional Layers Through Data and Experiments\n",
    "\n",
    "**Dataset:** Fashion-MNIST\n",
    "\n",
    "**Experiment Focus:** Kernel Size Comparison (3×3 vs 5×5)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Dataset Exploration (EDA)](#1-dataset-exploration)\n",
    "2. [Baseline Model (Non-Convolutional)](#2-baseline-model)\n",
    "3. [Convolutional Architecture Design](#3-convolutional-architecture)\n",
    "4. [Controlled Experiments](#4-controlled-experiments)\n",
    "5. [Interpretation and Reasoning](#5-interpretation)\n",
    "6. [SageMaker Deployment](#6-sagemaker-deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Style settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Dataset Exploration (EDA)\n",
    "\n",
    "### 1.1 Dataset Selection Justification\n",
    "\n",
    "**Why Fashion-MNIST is appropriate for convolutional layers:**\n",
    "\n",
    "Fashion-MNIST consists of grayscale images of clothing items, where:\n",
    "- **Spatial structure matters**: Clothing items have recognizable shapes and textures that are spatially organized\n",
    "- **Translation invariance**: A shirt is a shirt regardless of its exact position in the image\n",
    "- **Local patterns**: Features like collars, sleeves, and patterns are localized and can be detected with small kernels\n",
    "- **Hierarchical features**: Low-level edges combine into mid-level textures, which form high-level garment shapes\n",
    "\n",
    "This makes it ideal for demonstrating how convolutional layers exploit spatial structure through shared weights and local receptive fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"\\nNumber of classes: {len(class_names)}\")\n",
    "print(f\"Pixel value range: [{X_train.min()}, {X_train.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples per class\n",
    "train_counts = Counter(y_train)\n",
    "test_counts = Counter(y_test)\n",
    "\n",
    "# Create distribution dataframe\n",
    "dist_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Train': [train_counts[i] for i in range(10)],\n",
    "    'Test': [test_counts[i] for i in range(10)]\n",
    "})\n",
    "\n",
    "print(dist_df)\n",
    "print(f\"\\nDataset is balanced: {len(set(train_counts.values())) == 1}\")\n",
    "\n",
    "# Visualize distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax[0].bar(range(10), [train_counts[i] for i in range(10)], color='steelblue', alpha=0.7)\n",
    "ax[0].set_xticks(range(10))\n",
    "ax[0].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax[0].set_ylabel('Number of samples')\n",
    "ax[0].set_title('Training Set Distribution')\n",
    "ax[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax[1].bar(range(10), [test_counts[i] for i in range(10)], color='coral', alpha=0.7)\n",
    "ax[1].set_xticks(range(10))\n",
    "ax[1].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax[1].set_ylabel('Number of samples')\n",
    "ax[1].set_title('Test Set Distribution')\n",
    "ax[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Sample Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    # Find first occurrence of each class\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    axes[i].imshow(X_train[idx], cmap='gray')\n",
    "    axes[i].set_title(f'{class_names[i]}\\n(Class {i})')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images per Class', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Image Statistics and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pixel intensity distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Overall pixel distribution\n",
    "axes[0].hist(X_train.flatten(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Pixel Intensity')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Pixel Intensity Distribution (Training Set)')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Mean pixel value per class\n",
    "mean_intensities = [X_train[y_train == i].mean() for i in range(10)]\n",
    "axes[1].bar(range(10), mean_intensities, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xticks(range(10))\n",
    "axes[1].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Mean Pixel Intensity')\n",
    "axes[1].set_title('Average Brightness per Class')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Overall mean pixel value: {X_train.mean():.2f}\")\n",
    "print(f\"Overall std pixel value: {X_train.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1]\n",
    "X_train_normalized = X_train.astype('float32') / 255.0\n",
    "X_test_normalized = X_test.astype('float32') / 255.0\n",
    "\n",
    "# For CNN: reshape to add channel dimension (28, 28, 1)\n",
    "X_train_cnn = X_train_normalized.reshape(-1, 28, 28, 1)\n",
    "X_test_cnn = X_test_normalized.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# For baseline (fully connected): flatten to 1D vector\n",
    "X_train_flat = X_train_normalized.reshape(-1, 28*28)\n",
    "X_test_flat = X_test_normalized.reshape(-1, 28*28)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n",
    "\n",
    "print(\"Preprocessed shapes:\")\n",
    "print(f\"CNN input: {X_train_cnn.shape}\")\n",
    "print(f\"Flattened input: {X_train_flat.shape}\")\n",
    "print(f\"Labels (one-hot): {y_train_cat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Baseline Model (Non-Convolutional)\n",
    "\n",
    "### 2.1 Architecture Definition\n",
    "\n",
    "The baseline model uses only fully connected (Dense) layers:\n",
    "- **Input**: Flattened 784-dimensional vector (28×28)\n",
    "- **Hidden layers**: Two dense layers with ReLU activation\n",
    "- **Output**: 10-class softmax\n",
    "\n",
    "This architecture ignores spatial structure and treats all pixels equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model():\n",
    "    \"\"\"\n",
    "    Fully connected baseline network.\n",
    "    No convolutional layers - treats input as flat vector.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        \n",
    "        # First dense layer\n",
    "        layers.Dense(128, activation='relu', name='dense1'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Second dense layer\n",
    "        layers.Dense(64, activation='relu', name='dense2'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(10, activation='softmax', name='output')\n",
    "    ], name='Baseline_FC')\n",
    "    \n",
    "    return model\n",
    "\n",
    "baseline_model = create_baseline_model()\n",
    "baseline_model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters: {baseline_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history_baseline = baseline_model.fit(\n",
    "    X_train_flat, y_train_cat,\n",
    "    validation_split=0.1,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "baseline_loss, baseline_acc = baseline_model.evaluate(X_test_flat, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"Baseline Model Performance:\")\n",
    "print(f\"Test Accuracy: {baseline_acc*100:.2f}%\")\n",
    "print(f\"Test Loss: {baseline_loss:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(history_baseline.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0].plot(history_baseline.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Baseline Model - Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history_baseline.history['loss'], label='Train', linewidth=2)\n",
    "axes[1].plot(history_baseline.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Baseline Model - Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Observed Limitations\n",
    "\n",
    "**Key limitations of the fully connected baseline:**\n",
    "\n",
    "1. **Ignores spatial structure**: Flattening destroys the 2D relationships between pixels\n",
    "2. **No translation invariance**: A pattern must be learned separately for each position\n",
    "3. **Parameter explosion**: Every pixel connects to every neuron (784 × 128 = 100,352 weights in first layer alone)\n",
    "4. **No hierarchical learning**: Cannot build complex features from simpler ones\n",
    "5. **Overfitting risk**: Large parameter count relative to training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Convolutional Architecture Design\n",
    "\n",
    "### 3.1 Architecture Rationale\n",
    "\n",
    "**Design principles:**\n",
    "\n",
    "1. **Small kernels (3×3)**: Capture local patterns efficiently while allowing deep networks\n",
    "2. **Progressive channel growth**: 32 → 64 → 128 filters to learn increasingly complex features\n",
    "3. **Max pooling**: Downsample spatial dimensions for translation invariance and computational efficiency\n",
    "4. **Moderate depth**: 3 convolutional blocks balance expressiveness with training stability\n",
    "5. **ReLU activation**: Standard choice for non-linearity in CNNs\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (28×28×1)\n",
    "    ↓\n",
    "Conv2D(32, 3×3) + ReLU → (28×28×32)\n",
    "MaxPool(2×2) → (14×14×32)\n",
    "    ↓\n",
    "Conv2D(64, 3×3) + ReLU → (14×14×64)\n",
    "MaxPool(2×2) → (7×7×64)\n",
    "    ↓\n",
    "Conv2D(128, 3×3) + ReLU → (7×7×128)\n",
    "MaxPool(2×2) → (3×3×128)\n",
    "    ↓\n",
    "Flatten → (1152)\n",
    "    ↓\n",
    "Dense(128) + ReLU + Dropout\n",
    "    ↓\n",
    "Dense(10) + Softmax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(kernel_size=3, name='CNN'):\n",
    "    \"\"\"\n",
    "    Convolutional neural network for Fashion-MNIST.\n",
    "    \n",
    "    Args:\n",
    "        kernel_size: Size of convolutional kernels (3 or 5)\n",
    "        name: Model name for identification\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(28, 28, 1)),\n",
    "        \n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (kernel_size, kernel_size), \n",
    "                     activation='relu', \n",
    "                     padding='same',\n",
    "                     name=f'conv1_{kernel_size}x{kernel_size}'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (kernel_size, kernel_size), \n",
    "                     activation='relu', \n",
    "                     padding='same',\n",
    "                     name=f'conv2_{kernel_size}x{kernel_size}'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        layers.Conv2D(128, (kernel_size, kernel_size), \n",
    "                     activation='relu', \n",
    "                     padding='same',\n",
    "                     name=f'conv3_{kernel_size}x{kernel_size}'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool3'),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Flatten(name='flatten'),\n",
    "        layers.Dense(128, activation='relu', name='dense1'),\n",
    "        layers.Dropout(0.3, name='dropout'),\n",
    "        layers.Dense(10, activation='softmax', name='output')\n",
    "    ], name=name)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and display model with 3x3 kernels\n",
    "cnn_3x3 = create_cnn_model(kernel_size=3, name='CNN_3x3')\n",
    "cnn_3x3.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters: {cnn_3x3.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Architectural Justifications\n",
    "\n",
    "**Padding = 'same':**\n",
    "- Preserves spatial dimensions after convolution\n",
    "- Allows information at image borders to be processed equally\n",
    "- More stable training with multiple layers\n",
    "\n",
    "**MaxPooling 2×2:**\n",
    "- Reduces spatial dimensions by half\n",
    "- Provides translation invariance\n",
    "- Reduces computational cost\n",
    "- Acts as mild regularization\n",
    "\n",
    "**Progressive filter increase (32→64→128):**\n",
    "- Early layers learn simple features (edges, textures)\n",
    "- Later layers combine these into complex patterns\n",
    "- Matches the hierarchical nature of visual information\n",
    "\n",
    "**Dropout (0.3):**\n",
    "- Prevents overfitting on the dense layer\n",
    "- Not used in conv layers as they already have strong regularization through weight sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Controlled Experiments on Kernel Size\n",
    "\n",
    "### 4.1 Experiment Design\n",
    "\n",
    "**Research question:** How does kernel size affect model performance and feature learning?\n",
    "\n",
    "**Experimental setup:**\n",
    "- **Variable:** Kernel size (3×3 vs 5×5)\n",
    "- **Fixed:** All other hyperparameters (layers, filters, optimizer, learning rate, batch size)\n",
    "- **Metrics:** Accuracy, loss, training time, parameter count\n",
    "\n",
    "**Hypothesis:**\n",
    "- 3×3 kernels: Faster training, fewer parameters, may require more depth\n",
    "- 5×5 kernels: Larger receptive field per layer, more parameters, potentially better feature capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train Model with 3×3 Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile 3x3 model\n",
    "cnn_3x3.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "history_3x3 = cnn_3x3.fit(\n",
    "    X_train_cnn, y_train_cat,\n",
    "    validation_split=0.1,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "time_3x3 = time.time() - start_time\n",
    "print(f\"\\nTraining time (3×3): {time_3x3:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train Model with 5×5 Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile 5x5 model\n",
    "cnn_5x5 = create_cnn_model(kernel_size=5, name='CNN_5x5')\n",
    "cnn_5x5.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"5×5 Model Summary:\")\n",
    "cnn_5x5.summary()\n",
    "print(f\"\\nTotal parameters: {cnn_5x5.count_params():,}\")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "\n",
    "history_5x5 = cnn_5x5.fit(\n",
    "    X_train_cnn, y_train_cat,\n",
    "    validation_split=0.1,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "time_5x5 = time.time() - start_time\n",
    "print(f\"\\nTraining time (5×5): {time_5x5:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Quantitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models\n",
    "loss_3x3, acc_3x3 = cnn_3x3.evaluate(X_test_cnn, y_test_cat, verbose=0)\n",
    "loss_5x5, acc_5x5 = cnn_5x5.evaluate(X_test_cnn, y_test_cat, verbose=0)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Test Accuracy (%)', 'Test Loss', 'Parameters', 'Training Time (s)', 'Params per Layer 1'],\n",
    "    'Baseline (FC)': [\n",
    "        f\"{baseline_acc*100:.2f}\",\n",
    "        f\"{baseline_loss:.4f}\",\n",
    "        f\"{baseline_model.count_params():,}\",\n",
    "        'N/A',\n",
    "        'N/A'\n",
    "    ],\n",
    "    'CNN 3×3': [\n",
    "        f\"{acc_3x3*100:.2f}\",\n",
    "        f\"{loss_3x3:.4f}\",\n",
    "        f\"{cnn_3x3.count_params():,}\",\n",
    "        f\"{time_3x3:.2f}\",\n",
    "        f\"{32 * 3 * 3 + 32:,}\"  # filters * kernel_h * kernel_w + bias\n",
    "    ],\n",
    "    'CNN 5×5': [\n",
    "        f\"{acc_5x5*100:.2f}\",\n",
    "        f\"{loss_5x5:.4f}\",\n",
    "        f\"{cnn_5x5.count_params():,}\",\n",
    "        f\"{time_5x5:.2f}\",\n",
    "        f\"{32 * 5 * 5 + 32:,}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENTAL RESULTS: KERNEL SIZE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Training Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of training dynamics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training accuracy\n",
    "axes[0, 0].plot(history_3x3.history['accuracy'], label='3×3 Train', linewidth=2)\n",
    "axes[0, 0].plot(history_5x5.history['accuracy'], label='5×5 Train', linewidth=2, linestyle='--')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Training Accuracy Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "axes[0, 1].plot(history_3x3.history['val_accuracy'], label='3×3 Validation', linewidth=2)\n",
    "axes[0, 1].plot(history_5x5.history['val_accuracy'], label='5×5 Validation', linewidth=2, linestyle='--')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Validation Accuracy Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Training loss\n",
    "axes[1, 0].plot(history_3x3.history['loss'], label='3×3 Train', linewidth=2)\n",
    "axes[1, 0].plot(history_5x5.history['loss'], label='5×5 Train', linewidth=2, linestyle='--')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Training Loss Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "axes[1, 1].plot(history_3x3.history['val_loss'], label='3×3 Validation', linewidth=2)\n",
    "axes[1, 1].plot(history_5x5.history['val_loss'], label='5×5 Validation', linewidth=2, linestyle='--')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].set_title('Validation Loss Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Qualitative Observations\n",
    "\n",
    "**Performance trade-offs:**\n",
    "\n",
    "1. **Accuracy**: [To be filled after running]\n",
    "2. **Parameter efficiency**: 3×3 kernels use ~2.8× fewer parameters per layer than 5×5\n",
    "3. **Training speed**: [To be compared after running]\n",
    "4. **Convergence**: [To be observed from curves]\n",
    "\n",
    "**Key insights:**\n",
    "- Smaller kernels allow for deeper networks with the same parameter budget\n",
    "- 3×3 kernels are the modern standard (VGG, ResNet) as they can approximate larger receptive fields through stacking\n",
    "- 5×5 kernels capture more context in a single layer but with higher computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Feature Visualization (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned filters from first convolutional layer\n",
    "def visualize_filters(model, layer_name, title):\n",
    "    # Get weights from first conv layer\n",
    "    layer = model.get_layer(layer_name)\n",
    "    filters, biases = layer.get_weights()\n",
    "    \n",
    "    # Normalize filters for visualization\n",
    "    f_min, f_max = filters.min(), filters.max()\n",
    "    filters = (filters - f_min) / (f_max - f_min)\n",
    "    \n",
    "    # Plot first 32 filters\n",
    "    n_filters = min(32, filters.shape[3])\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(n_filters):\n",
    "        axes[i].imshow(filters[:, :, 0, i], cmap='viridis')\n",
    "        axes[i].set_title(f'Filter {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize both models\n",
    "visualize_filters(cnn_3x3, 'conv1_3x3', 'Learned Filters: 3×3 Kernels (First Layer)')\n",
    "visualize_filters(cnn_5x5, 'conv1_5x5', 'Learned Filters: 5×5 Kernels (First Layer)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Interpretation and Architectural Reasoning\n",
    "\n",
    "### 5.1 Why Did Convolutional Layers Outperform the Baseline?\n",
    "\n",
    "Convolutional layers achieved superior performance due to several key advantages:\n",
    "\n",
    "**1. Spatial Inductive Bias:**\n",
    "- CNNs preserve and exploit the 2D spatial structure of images\n",
    "- Neighboring pixels are processed together, respecting their local relationships\n",
    "- This matches the natural structure of visual data where nearby pixels are highly correlated\n",
    "\n",
    "**2. Parameter Sharing (Weight Reuse):**\n",
    "- The same filter is applied across all spatial locations\n",
    "- Drastically reduces parameters: a 3×3 conv layer with 32 filters has only 320 parameters, while connecting 784 inputs to 32 outputs fully would require 25,088 parameters\n",
    "- This sharing acts as strong regularization, reducing overfitting\n",
    "\n",
    "**3. Translation Invariance:**\n",
    "- A feature detector (e.g., edge detector) works regardless of where the edge appears in the image\n",
    "- The model doesn't need to learn \"sleeve at position (10,10)\" and \"sleeve at position (15,15)\" separately\n",
    "- This is crucial for clothing recognition where garments can appear at different positions\n",
    "\n",
    "**4. Hierarchical Feature Learning:**\n",
    "- Early layers detect simple features (edges, textures)\n",
    "- Middle layers combine these into parts (collars, pockets, straps)\n",
    "- Deep layers recognize complete objects (shirts, shoes)\n",
    "- This compositional structure mirrors how humans perceive visual information\n",
    "\n",
    "**5. Local Receptive Fields:**\n",
    "- Each neuron only looks at a small patch of the input\n",
    "- Reduces the search space and makes learning more tractable\n",
    "- Deeper layers progressively see larger regions through composition\n",
    "\n",
    "### 5.2 What Inductive Bias Does Convolution Introduce?\n",
    "\n",
    "**Definition:** An inductive bias is a set of assumptions that constrain the hypothesis space, guiding learning toward solutions that work well for specific problem types.\n",
    "\n",
    "**Convolutional layers introduce three main biases:**\n",
    "\n",
    "**1. Locality:**\n",
    "- Assumption: Relevant features are found in local spatial neighborhoods\n",
    "- Implementation: Small receptive fields (3×3, 5×5)\n",
    "- Implication: Distant pixels are not directly connected in early layers\n",
    "\n",
    "**2. Stationarity (Translation Equivariance):**\n",
    "- Assumption: Useful features can appear anywhere in the image\n",
    "- Implementation: Weight sharing across spatial locations\n",
    "- Implication: The same pattern detector is reused across the entire image\n",
    "\n",
    "**3. Hierarchical Composition:**\n",
    "- Assumption: Complex patterns are built from simpler ones\n",
    "- Implementation: Stacked convolutional layers with progressive abstraction\n",
    "- Implication: The model learns a compositional representation\n",
    "\n",
    "These biases are highly beneficial for images but would be inappropriate for other data types.\n",
    "\n",
    "### 5.3 In What Types of Problems Would Convolution NOT Be Appropriate?\n",
    "\n",
    "**1. Tabular/Structured Data:**\n",
    "- **Example:** Predicting customer churn from age, income, account balance\n",
    "- **Why not:** No spatial structure; the order of features is arbitrary\n",
    "- **Better choice:** Fully connected networks, gradient boosting trees\n",
    "\n",
    "**2. Sequential Data Without Local Patterns:**\n",
    "- **Example:** Natural language where long-range dependencies matter more than local n-grams\n",
    "- **Why not:** Important relationships may span hundreds of tokens\n",
    "- **Better choice:** Transformers with global attention\n",
    "\n",
    "**3. Data Where Position Carries Semantic Meaning:**\n",
    "- **Example:** Chess board state where \"pawn at position e4\" has different meaning than \"pawn at position a7\"\n",
    "- **Why not:** Translation invariance is harmful when absolute position matters\n",
    "- **Better choice:** Positional embeddings + attention mechanisms\n",
    "\n",
    "**4. Irregular Graphs and Point Clouds:**\n",
    "- **Example:** Social networks, molecular structures, 3D object point clouds\n",
    "- **Why not:** No regular grid structure; neighbors are not spatially organized\n",
    "- **Better choice:** Graph neural networks, point cloud networks\n",
    "\n",
    "**5. Problems Requiring Global Context Immediately:**\n",
    "- **Example:** Determining if a very long document discusses politics (could be mentioned anywhere)\n",
    "- **Why not:** Local receptive fields require many layers to aggregate global information\n",
    "- **Better choice:** Attention mechanisms that can look at entire input at once\n",
    "\n",
    "**6. Very Small Datasets:**\n",
    "- **Example:** Medical diagnosis with only 50 examples\n",
    "- **Why not:** Even with parameter sharing, CNNs may overfit severely\n",
    "- **Better choice:** Pre-trained models with transfer learning, or classical ML\n",
    "\n",
    "**General Rule:** Use convolutions when:\n",
    "- Data has grid-like topology (images, audio spectrograms, video)\n",
    "- Local patterns are meaningful\n",
    "- Translation invariance is desired\n",
    "- Hierarchical composition makes sense\n",
    "\n",
    "Avoid convolutions when these properties don't hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. SageMaker Deployment\n",
    "\n",
    "### 6.1 Prepare Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "best_model = cnn_3x3  # or cnn_5x5 depending on results\n",
    "model_path = 'fashion_mnist_cnn'\n",
    "best_model.save(model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 SageMaker Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"Default bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Create Training Script for SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def create_cnn_model(kernel_size=3):\n",
    "    \"\"\"Create CNN model\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(28, 28, 1)),\n",
    "        \n",
    "        layers.Conv2D(32, (kernel_size, kernel_size), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(64, (kernel_size, kernel_size), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(128, (kernel_size, kernel_size), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Hyperparameters\n",
    "    parser.add_argument('--epochs', type=int, default=15)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--kernel-size', type=int, default=3)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.001)\n",
    "    \n",
    "    # SageMaker specific arguments\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = create_cnn_model(kernel_size=args.kernel_size)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=args.learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.1,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'Test loss: {score[0]:.4f}')\n",
    "    print(f'Test accuracy: {score[1]:.4f}')\n",
    "    \n",
    "    # Save model\n",
    "    model.save(os.path.join(args.model_dir, '1'))\n",
    "    print(f\"Model saved to {args.model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Train on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow estimator\n",
    "tf_estimator = TensorFlow(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',  # GPU instance\n",
    "    framework_version='2.12',\n",
    "    py_version='py310',\n",
    "    hyperparameters={\n",
    "        'epochs': 15,\n",
    "        'batch-size': 128,\n",
    "        'kernel-size': 3,\n",
    "        'learning-rate': 0.001\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start training\n",
    "tf_estimator.fit()\n",
    "\n",
    "print(\"Training job completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Training Limitation\n",
    "\n",
    "Although the TensorFlow estimator is correctly defined, the execution of the training job (CreateTrainingJob) is restricted by the IAM policies of the lab environment.\n",
    "Therefore, the training and deployment steps are provided for completeness and conceptual demonstration, but were not executed to avoid unauthorized cloud operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Deploy to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to endpoint\n",
    "predictor = tf_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name='fashion-mnist-cnn-endpoint'\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction with a sample image\n",
    "test_image = X_test_cnn[0:1]  # Take first test image\n",
    "true_label = class_names[y_test[0]]\n",
    "\n",
    "# Make prediction\n",
    "prediction = predictor.predict(test_image)\n",
    "predicted_class = np.argmax(prediction['predictions'][0])\n",
    "predicted_label = class_names[predicted_class]\n",
    "confidence = prediction['predictions'][0][predicted_class]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title(f'True: {true_label}\\nPredicted: {predicted_label}\\nConfidence: {confidence:.2%}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEndpoint is working correctly!\")\n",
    "print(f\"Endpoint name: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete endpoint and avoid charges\n",
    "# predictor.delete_endpoint()\n",
    "# print(\"Endpoint deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Dataset Understanding**: Fashion-MNIST provides an ideal testbed for convolutional architectures due to its spatial structure and translation-invariant patterns\n",
    "\n",
    "2. **Baseline Comparison**: Fully connected networks achieved reasonable performance but are fundamentally limited by their inability to exploit spatial structure\n",
    "\n",
    "3. **CNN Advantages**: Convolutional layers demonstrated clear benefits through parameter sharing, translation invariance, and hierarchical feature learning\n",
    "\n",
    "4. **Kernel Size Experiment**: [Summarize findings from 3×3 vs 5×5 comparison]\n",
    "\n",
    "5. **Architectural Insights**: The choice of convolutional architecture should be guided by the inductive biases appropriate to the problem domain\n",
    "\n",
    "### Lessons Learned:\n",
    "\n",
    "- **Inductive bias matters**: Architectural choices encode assumptions about the data structure\n",
    "- **Simplicity often wins**: 3×3 kernels are standard for good reason\n",
    "- **Understanding > hyperparameter tuning**: Knowing why an architecture works is more valuable than finding optimal parameters\n",
    "- **Deployment considerations**: SageMaker enables scalable model serving for production use\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "- Experiment with batch normalization for training stability\n",
    "- Try data augmentation (rotation, shift) to improve generalization\n",
    "- Implement residual connections for deeper networks\n",
    "- Compare with modern architectures (EfficientNet, Vision Transformer)\n",
    "- Deploy with auto-scaling for production workloads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
